{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom torch.utils.data import TensorDataset, Dataset\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport copy\nimport csv\nimport numpy as np\nimport math\nfrom collections import defaultdict, Counter\nfrom tqdm.notebook import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Subset\nimport torchvision # <-- Added torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms # <-- Clarified import\nfrom torch.utils.data import random_split\nfrom PIL import Image\nimport itertools\n\n# --- Scikit-learn ---\nfrom sklearn.metrics import f1_score, precision_score, recall_score, classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# =============================================================================\n# UTILITY FUNCTIONS FOR DATA HANDLING\n# =============================================================================\n\ndef get_subset_labels(subset):\n    \"\"\"Safely extracts labels from a PyTorch Subset object.\"\"\"\n    base_dataset = subset.dataset\n    if hasattr(base_dataset, 'targets'):\n        all_targets = np.array(base_dataset.targets)\n    elif hasattr(base_dataset, 'samples'):\n        all_targets = np.array([s[1] for s in base_dataset.samples])\n    elif hasattr(base_dataset, 'labels'): # Added for placeholder/other datasets\n        all_targets = np.array(base_dataset.labels)\n    else:\n        raise AttributeError(\"Underlying dataset lacks 'targets', 'samples', or 'labels' attribute for label access.\")\n    # Filter the labels using the subset's indices\n    subset_labels = all_targets[subset.indices]\n    return subset_labels.squeeze()\n\n\ndef create_median_balanced_subset(val_dataset, num_classes, random_seed=42):\n    \"\"\"Balances a Subset using median class count sampling.\"\"\"\n    # Ensure it's treated as a Subset for consistent access\n    if not isinstance(val_dataset, Subset):\n        val_dataset = Subset(val_dataset, range(len(val_dataset)))\n\n    y_val = get_subset_labels(val_dataset)\n\n    # 1. Calculate counts and median\n    class_indices_val = {i: np.where(y_val == i)[0] for i in range(num_classes)}\n    counts_list = [len(indices) for indices in class_indices_val.values() if len(indices) > 0] # Avoid empty classes\n    if not counts_list:\n        print(\"Warning: No samples found in validation subset for balancing.\")\n        return val_dataset # Return original if empty\n    median_count = int(np.median(counts_list))\n    # Ensure median_count is at least 1 if dataset is extremely small/skewed\n    if median_count == 0 and len(val_dataset) > 0:\n         print(f\"Warning: Median count is 0. Setting target count to 1.\")\n         median_count = 1\n    elif median_count == 0:\n        print(\"Warning: Median count is 0 and validation set seems empty. Returning original.\")\n        return val_dataset\n\n    print(f\"Original validation counts (subset): {[len(class_indices_val.get(i, [])) for i in range(num_classes)]}. Median target: {median_count}.\")\n\n    np.random.seed(random_seed)\n\n    # 2. Sample to median count\n    balanced_indices_absolute = []\n    # Map from subset index -> original dataset index\n    absolute_indices_map = np.array(val_dataset.indices)\n\n    for i in range(num_classes):\n        # Indices relative to the subset\n        indices_in_subset = class_indices_val.get(i, np.array([])) # Use .get for safety\n        num_samples = len(indices_in_subset)\n\n        if num_samples == 0: continue # Skip classes not present in the subset\n\n        # Sample indices relative to the subset\n        new_class_indices_in_subset = np.random.choice(\n            indices_in_subset,\n            size=median_count,\n            replace=(num_samples < median_count) # Oversample if needed\n        )\n\n        # Map the relative indices back to the original parent dataset's absolute indices\n        abs_indices_to_sample = absolute_indices_map[new_class_indices_in_subset]\n        balanced_indices_absolute.extend(abs_indices_to_sample)\n\n    # 3. Create the new Subset from the original parent dataset\n    parent_dataset = val_dataset.dataset\n    if not balanced_indices_absolute:\n         print(\"Warning: No indices selected after balancing. Returning original subset.\")\n         return val_dataset\n    new_val_dataset = Subset(parent_dataset, balanced_indices_absolute)\n\n    print(f\"Created a new balanced validation dataset with {len(new_val_dataset)} total samples.\")\n\n    return new_val_dataset\n\n\n# =============================================================================\n# FEDERATED SPLIT FUNCTION (Modified for .targets)\n# =============================================================================\ndef create_non_iid_split(train_dataset, num_clients, num_classes, alpha):\n    \"\"\"\n    Creates a non-IID data split using Dirichlet distribution.\n    Adapted for datasets using the .targets attribute (like torchvision CIFAR10).\n    \"\"\"\n    # Use the 'targets' attribute from the underlying dataset\n    if isinstance(train_dataset, Subset):\n        underlying_dataset = train_dataset.dataset\n        subset_indices = train_dataset.indices\n        if hasattr(underlying_dataset, 'targets'):\n            all_labels = np.array(underlying_dataset.targets)\n        else:\n            raise AttributeError(\"Underlying dataset does not have 'targets' attribute.\")\n        labels_in_subset = all_labels[subset_indices]\n        # We are splitting the subset itself, so indices are relative to the subset\n        indices_map_to_subset = np.arange(len(subset_indices))\n        dataset_to_split = train_dataset\n\n    else: # If the input is a full dataset\n        if hasattr(train_dataset, 'targets'):\n            labels_in_subset = np.array(train_dataset.targets)\n        else:\n            raise AttributeError(\"Dataset does not have 'targets' attribute.\")\n        indices_map_to_subset = np.arange(len(train_dataset)) # Indices are 0 to N-1\n        dataset_to_split = train_dataset\n\n    labels_in_subset = labels_in_subset.squeeze()\n\n    # Get indices for each class *within the current subset*\n    class_indices_relative = [np.where(labels_in_subset == i)[0] for i in range(num_classes)]\n\n    client_indices_relative = defaultdict(list)\n    client_labels_dict = defaultdict(list) # To store labels per client\n\n    for class_idx, relative_indices in enumerate(class_indices_relative):\n        if len(relative_indices) == 0:\n            continue\n\n        # Distribute these relative indices among clients\n        proportions = np.random.dirichlet(np.repeat(alpha, num_clients))\n        # Ensure every client gets at least one sample if possible, while handling 0-sum\n        proportions = np.maximum(proportions * len(relative_indices), 1).astype(int)\n        proportions = proportions / proportions.sum() # Normalize\n        proportions = (proportions * len(relative_indices)).astype(int)\n        \n        # Handle remainder\n        diff = len(relative_indices) - proportions.sum()\n        for i in range(diff):\n            proportions[i % num_clients] += 1\n\n        np.random.shuffle(relative_indices)\n        current_idx = 0\n        for client_id in range(num_clients):\n            split_size = proportions[client_id]\n            assigned_relative_indices = relative_indices[current_idx : current_idx + split_size]\n            client_indices_relative[f\"client_{client_id+1}\"].extend(assigned_relative_indices)\n\n            # Store labels for these assigned indices\n            assigned_labels = labels_in_subset[assigned_relative_indices].tolist()\n            client_labels_dict[f\"client_{client_id+1}\"].extend(assigned_labels)\n\n            current_idx += split_size\n\n    # Create the final Subset objects, mapping relative indices back to the original dataset\n    client_datasets = {}\n    for client_id, relative_indices in client_indices_relative.items():\n        # Map relative indices back to the indices within the input train_dataset\n        original_subset_indices = indices_map_to_subset[relative_indices]\n        # Create a Subset using the *original dataset object* (dataset_to_split)\n        client_datasets[client_id] = Subset(dataset_to_split, original_subset_indices)\n\n    return client_datasets, client_labels_dict\n\n\n# =============================================================================\n# MODEL CLASS (Simple 5-Layer CNN for PSFL Comparison)\n# =============================================================================\nclass Simple5LayerCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(Simple5LayerCNN, self).__init__()\n        # From PSFL paper: \"two 5x5 convolutional layers, each followed by ReLU and max pooling\"\n        # Input: 3x32x32\n        self.conv_layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5), # Output: (64, 28, 28)\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2) # Output: (64, 14, 14)\n        )\n        self.conv_layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=5), # Output: (64, 10, 10)\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2) # Output: (64, 5, 5)\n        )\n        \n        # From PSFL paper: \"and three fully connected layers\"\n        # Flattened size = 64 channels * 5 * 5 = 1600\n        self.fc_layer1 = nn.Sequential(\n            nn.Linear(64 * 5 * 5, 384),\n            nn.ReLU()\n        )\n        self.fc_layer2 = nn.Sequential(\n            nn.Linear(384, 192),\n            nn.ReLU()\n        )\n        self.fc_layer3 = nn.Linear(192, num_classes) # Final output layer\n\n    def forward(self, x):\n        x = self.conv_layer1(x)\n        x = self.conv_layer2(x)\n        x = torch.flatten(x, 1) # Flatten all dimensions except batch\n        x = self.fc_layer1(x)\n        x = self.fc_layer2(x)\n        x = self.fc_layer3(x)\n        return x\n\n# =============================================================================\n# FEDERATED TRAINER CLASS (Lorenzo - Conservative Jacobi + Bootstrap)\n# =============================================================================\nclass FederatedTrainer:\n    def __init__(self, config, train_dataset, val_dataset, test_dataset):\n        self.config = config\n        self.NUM_CLIENTS = config.get(\"NUM_CLIENTS\", 5)\n        self.NUM_GLOBAL_ITERATIONS = config.get(\"NUM_GLOBAL_ITERATIONS\", 5)\n        self.LOCAL_EPOCHS = config.get(\"LOCAL_EPOCHS\", 3)\n        self.BATCH_SIZE = config.get(\"BATCH_SIZE\", 32)\n        self.LEARNING_RATE = config.get(\"LEARNING_RATE\", 1e-3)\n        self.RANDOM_SEED = config.get(\"RANDOM_SEED\", 42)\n        self.EPSILON = config.get(\"EPSILON\", 1e-9)\n        self.NUM_CLASSES = config.get(\"NUM_CLASSES\", 10) # Default to 10 for CIFAR\n        self.ALPHA_VALUE = config.get(\"ALPHA_VALUE\", 0.4)\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        random.seed(self.RANDOM_SEED)\n        np.random.seed(self.RANDOM_SEED)\n        torch.manual_seed(self.RANDOM_SEED)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(self.RANDOM_SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        print(f\"Using device: {self.device}\")\n\n        # Get class names from config\n        self.class_names = config.get(\"CLASS_NAMES\", [])\n        if not self.class_names or len(self.class_names) != self.NUM_CLASSES:\n            print(f\"Warning: 'CLASS_NAMES' config missing or length != NUM_CLASSES ({self.NUM_CLASSES}). Generating generic names.\")\n            self.class_names = [f\"Class_{i}\" for i in range(self.NUM_CLASSES)]\n\n        self.train_dataset = train_dataset # This is the raw_train_dataset (e.g., 40k)\n        self.val_dataset = val_dataset     # This is the balanced validation set\n        self.test_dataset = test_dataset   # This is the original test set (e.g., 10k)\n        # Use the actual test_dataset for the final loader\n        self.final_test_loader = DataLoader(self.test_dataset, batch_size=self.BATCH_SIZE, shuffle=False)\n\n        # --- MODIFIED: Ensure model is created with Simple5LayerCNN ---\n        print(\"Initializing Simple5LayerCNN model...\")\n        # This model is simple and not pretrained\n        temp_model = Simple5LayerCNN(num_classes=self.NUM_CLASSES)\n        self.initial_model_weights = self.to_cpu_sd(temp_model.state_dict())\n        del temp_model\n        # Reusable model is also a fresh instance\n        self.reusable_model = Simple5LayerCNN(num_classes=self.NUM_CLASSES).to(self.device)\n        print(\"Simple5LayerCNN models initialized.\")\n\n    def _prepare_data_for_run(self):\n        \"\"\"Prepares client data: non-IID split + filtering.\"\"\"\n        print(f\"\\nCreating non-IID split for {self.NUM_CLIENTS} clients with alpha={self.ALPHA_VALUE}...\")\n        client_datasets, all_client_labels = create_non_iid_split(\n            self.train_dataset, self.NUM_CLIENTS, self.NUM_CLASSES, self.ALPHA_VALUE\n        )\n\n        MIN_SAMPLES = self.BATCH_SIZE * 2\n        print(f\"Filtering clients: minimum samples required = {MIN_SAMPLES}\")\n\n        client_dataloaders = {\n            k: DataLoader(ds, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n            for k, ds in client_datasets.items()\n            if len(ds) >= MIN_SAMPLES\n        }\n\n        active_client_ids = set(client_dataloaders.keys())\n        client_labels = {\n            cid: labels for cid, labels in all_client_labels.items()\n            if cid in active_client_ids\n        }\n\n        print(f\"Total clients created (pre-filter): {len(client_datasets)}\")\n        print(f\"Created {len(client_dataloaders)} active client dataloaders (>= {MIN_SAMPLES} samples).\")\n\n        # The val_dataset passed during __init__ is already the balanced one\n        global_val_loader = DataLoader(self.val_dataset, batch_size=self.BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n        return client_dataloaders, global_val_loader, client_labels\n\n    def _report_data_distribution(self, client_labels):\n        \"\"\"Reports data distribution for active clients.\"\"\"\n        print(\"\\n--- Initial Data Distribution (for active clients) ---\")\n        if not client_labels:\n             print(\"No active clients to report.\")\n             return\n        for client_id, labels in client_labels.items():\n            print(f\"\\nClient: {client_id} (Total: {len(labels)} images)\")\n            counts = Counter(labels)\n            if not counts:\n                print(\"  - No data assigned.\")\n                continue\n            for i in range(self.NUM_CLASSES):\n                class_name = self.class_names[i] if i < len(self.class_names) else f\"Class_{i}\"\n                count = counts.get(i, 0)\n                print(f\"  - Class {i} ({class_name}): {count} images\")\n        print(\"-\" * 48)\n\n\n    def to_cpu_sd(self, state_dict):\n        \"\"\"Moves a state dict to CPU.\"\"\"\n        return {k: v.detach().cpu() for k, v in state_dict.items()}\n\n    def _calculate_rare_classes(self, labels):\n        \"\"\"Calculates number of unique classes.\"\"\"\n        if not labels: return 0\n        return len(set(labels))\n\n    def _train_local_model(self, model, initial_weights, train_loader, client_id_str, epochs): # Removed current_round\n        \"\"\"Trains model locally.\"\"\"\n        try:\n            model.load_state_dict(initial_weights, strict=True)\n        except RuntimeError as e:\n            print(f\"Error loading state dict for {client_id_str}: {e}\")\n            return self.to_cpu_sd(model.state_dict()) # Return current weights if load fails\n\n        params_to_train = [p for p in model.parameters() if p.requires_grad]\n        if not params_to_train:\n             print(f\"Warning: No trainable parameters found for {client_id_str}.\")\n             return self.to_cpu_sd(model.state_dict())\n\n        # --- NEW: Optimizer settings for PSFL comparison ---\n        current_lr = self.LEARNING_RATE # Use fixed LR\n        \n        optimizer = optim.SGD(\n            params_to_train,\n            lr=current_lr,\n            momentum=0.0,      # Match PSFL paper\n            weight_decay=1e-4  # Match PSFL paper\n        )\n        loss_fn = nn.CrossEntropyLoss()\n\n        model.train() \n\n        for epoch in range(epochs):\n            running_loss = 0.0\n            num_batches = 0\n            local_pbar = tqdm(train_loader, desc=f\"  > {client_id_str} | Local Epoch {epoch+1}/{epochs}\", leave=False, dynamic_ncols=True)\n\n            for inputs, labels in local_pbar:\n                if inputs.size(0) <= 1: continue \n\n                inputs, labels = inputs.to(self.device), torch.as_tensor(labels).squeeze().long().to(self.device)\n\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                try:\n                    loss = loss_fn(outputs, labels)\n                    loss.backward()\n                    optimizer.step()\n\n                    running_loss += loss.item()\n                    num_batches += 1\n                    if num_batches > 0:\n                        local_pbar.set_postfix(loss=f\"{(running_loss / num_batches):.4f}\")\n                except Exception as e:\n                    print(f\"Error during training batch for {client_id_str}: {e}\")\n                    continue \n\n        return self.to_cpu_sd(model.state_dict())\n\n    def _evaluate_model(self, model, model_weights, loader, desc=\"Evaluating\"):\n        \"\"\"Evaluates model, returns metrics and predictions.\"\"\"\n        try:\n            model.load_state_dict(model_weights, strict=True)\n        except RuntimeError as e:\n             print(f\"Error loading state dict for evaluation ({desc}): {e}\")\n             return 0.0, 0.0, 0.0, 0.0, [], [] # Return default bad values\n\n        model.eval() # Set model to evaluation mode\n        all_labels, all_preds = [], []\n        correct, total = 0, 0\n        with torch.no_grad():\n            for inputs, labels in tqdm(loader, desc=desc, leave=False, dynamic_ncols=True):\n                inputs, labels_squeezed = inputs.to(self.device), torch.as_tensor(labels).squeeze().long().to(self.device)\n                try:\n                    outputs = model(inputs)\n                    _, predicted = torch.max(outputs.data, 1)\n\n                    all_labels.extend(labels_squeezed.cpu().numpy())\n                    all_preds.extend(predicted.cpu().numpy())\n                    total += labels_squeezed.size(0)\n                    correct += (predicted == labels_squeezed).sum().item()\n                except Exception as e:\n                     print(f\"Error during evaluation batch ({desc}): {e}\")\n                     continue # Skip batch on error\n\n        if total == 0: return 0.0, 0.0, 0.0, 0.0, [], []\n\n        # Ensure labels are integers\n        all_labels = np.array(all_labels).astype(int)\n        all_preds = np.array(all_preds).astype(int)\n\n        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n        acc = correct / total\n        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n        return f1, acc, precision, recall, all_labels.tolist(), all_preds.tolist()\n\n    def _plot_confusion_matrix(self, y_true, y_pred, title):\n        \"\"\"Plots confusion matrix.\"\"\"\n        if not y_true or not y_pred:\n            print(f\"Skipping confusion matrix for '{title}': No data.\")\n            return\n        try:\n            # Ensure labels are within the expected range for display_labels\n            y_true_safe = [label if 0 <= label < len(self.class_names) else -1 for label in y_true]\n            y_pred_safe = [label if 0 <= label < len(self.class_names) else -1 for label in y_pred]\n\n            cm = confusion_matrix(y_true_safe, y_pred_safe, labels=range(len(self.class_names)))\n            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.class_names)\n\n            fig, ax = plt.subplots(figsize=(8, 8)) # Adjusted size\n            disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation='vertical')\n            ax.set_title(title)\n            plt.tight_layout() # Adjust layout\n            plt.show()\n        except Exception as e:\n            print(f\"Error plotting confusion matrix for '{title}': {e}\")\n\n\n    def _aggregate_weights(self, list_of_weights, coefficients):\n        \"\"\"Performs weighted averaging, handles potential Nones and non-float tensors.\"\"\"\n        # Filter out None weights and their corresponding coefficients\n        valid_weights = []\n        valid_coeffs = []\n        for w, c in zip(list_of_weights, coefficients):\n            if w is not None:\n                valid_weights.append(w)\n                valid_coeffs.append(c)\n\n        if not valid_weights: return None # Return None if no valid weights\n\n        sum_coeffs = sum(valid_coeffs)\n        # Handle zero sum coeffs - return average or first valid weight\n        if sum_coeffs == 0:\n            if not valid_weights: return None\n            # Fallback: simple average (equal weight) if scores are all zero\n            print(\"Warning: Sum of aggregation coefficients is zero. Using equal weights.\")\n            num_valid = len(valid_weights)\n            norm_coeffs = [1.0 / num_valid] * num_valid\n        else:\n            norm_coeffs = [c / sum_coeffs for c in valid_coeffs]\n\n        # Use the structure of the first valid weight as the template\n        agg_weights = copy.deepcopy(valid_weights[0])\n\n        # Iterate through keys and aggregate ONLY floating point tensors\n        for k in agg_weights.keys():\n            # Check if the tensor in the template is a floating point type\n            if agg_weights[k].dtype.is_floating_point:\n                # Accumulate weighted sum for this key\n                temp_accumulator = torch.zeros_like(agg_weights[k], dtype=torch.float32) # Use float32 for accumulation precision\n\n                for i, w in enumerate(valid_weights):\n                    if k in w and w[k] is not None:\n                        # Also check if the tensor from the source is float\n                        if w[k].dtype.is_floating_point:\n                             temp_accumulator += float(norm_coeffs[i]) * w[k].to(temp_accumulator.dtype).to(temp_accumulator.device)\n                        else:\n                             print(f\"Warning: Skipping non-float tensor '{k}' from model {i} during float aggregation.\")\n                    else:\n                         print(f\"Warning: Key '{k}' not found or tensor is None in weight set {i} during aggregation.\")\n\n                # Assign accumulated value back, casting to the original key's dtype\n                agg_weights[k] = temp_accumulator.to(agg_weights[k].dtype)\n\n            else:\n                # For non-floating point tensors (e.g., int/long buffers)\n                # Simply keep the value from the first model (which was already copied by deepcopy)\n                pass\n\n        return agg_weights\n\n\n    def train(self):\n        \"\"\"Main training loop implementing Lorenzo (Conservative Jacobi + Bootstrap).\"\"\"\n        print(f\"\\n--- Starting new run (Lorenzo): {self.NUM_CLIENTS} clients, alpha={self.ALPHA_VALUE} ---\")\n        client_dataloaders, global_val_loader, client_labels = self._prepare_data_for_run()\n\n        self._report_data_distribution(client_labels)\n\n        client_ids = list(client_dataloaders.keys())\n        if not client_ids:\n            print(\"No clients met the minimum sample threshold. Halting run.\")\n            return\n\n        W_global_prev = self.initial_model_weights\n\n        # --- BOOTSTRAP \"ROUND 0\" ---\n        print(f\"\\n{'='*30} BOOTSTRAP 'ROUND 0' {'='*30}\")\n        print(\"Step A: Running initial local training...\")\n        historical_local_weights = {}\n        bootstrap_pbar = tqdm(client_ids, desc=\"Bootstrap Training\", dynamic_ncols=True)\n        for cid in bootstrap_pbar:\n            bootstrap_pbar.set_description(f\"Bootstrap Training {cid}\")\n            # --- MODIFIED: Removed current_round ---\n            w_bootstrapped = self._train_local_model(\n                self.reusable_model,\n                self.initial_model_weights,\n                client_dataloaders[cid],\n                cid,\n                epochs=1\n            )\n            historical_local_weights[cid] = w_bootstrapped\n\n        print(\"\\nStep B: Evaluating bootstrapped models...\")\n        bootstrap_scores = {}\n        for cid, w in historical_local_weights.items():\n            if w is None: continue # Skip if bootstrap training failed\n            f1, acc, _, _, _, _ = self._evaluate_model(\n                self.reusable_model, w, global_val_loader, f\"Eval Bootstrap {cid}\"\n            )\n            bootstrap_scores[cid] = f1\n            print(f\"  - {cid}: F1={f1:.4f}, Acc={acc*100:.2f}%\")\n\n        print(\"\\nStep C: Aggregating to create initial 'Global Model 0.5'...\")\n        bootstrapped_weights_list = [historical_local_weights.get(cid) for cid in client_ids] # Use .get\n        bootstrapped_coeffs = [bootstrap_scores.get(cid, 0.0) for cid in client_ids] # Use .get with default 0\n\n        W_global_prev = self._aggregate_weights(bootstrapped_weights_list, bootstrapped_coeffs)\n        if W_global_prev is None:\n             print(\"Error: Aggregation failed during bootstrap. Using initial pretrained model.\")\n             W_global_prev = self.initial_model_weights\n        else:\n            iter_f1, iter_acc, _, _, _, _ = self._evaluate_model(\n                self.reusable_model, W_global_prev, global_val_loader, \"Eval Global 0.5\"\n            )\n            print(f\"-> Initial 'Global Model 0.5' created: F1={iter_f1:.4f}, Accuracy={iter_acc*100:.2f}%\")\n\n        print(f\"\\n{'='*30} BOOTSTRAP 'ROUND 0' COMPLETE {'='*30}\")\n\n        # --- MAIN TRAINING LOOP ---\n        for iteration in range(self.NUM_GLOBAL_ITERATIONS):\n            print(f\"\\n{'='*30} GLOBAL ITERATION {iteration + 1}/{self.NUM_GLOBAL_ITERATIONS} {'='*30}\")\n\n            print(\"Step 1: Evaluating and scoring clients...\")\n            client_scores = {}\n            for cid in client_ids:\n                w_local_prev = historical_local_weights.get(cid) # Use .get\n                if w_local_prev is None:\n                    print(f\"Warning: No historical weights for {cid}, assigning score 0.\")\n                    client_scores[cid] = 0.0\n                    continue\n                f1, acc, _, _, _, _ = self._evaluate_model(self.reusable_model, w_local_prev, global_val_loader, f\"Eval {cid}\")\n                rare_classes = self._calculate_rare_classes(client_labels.get(cid, [])) # Use .get\n                score = f1\n                client_scores[cid] = score\n                print(f\"  - {cid}: Acc={acc:.3f}, F1={f1:.3f}, r={rare_classes} -> Score={score:.4f}\")\n\n            active_scores = {cid: score for cid, score in client_scores.items()}\n            if not active_scores or sum(active_scores.values()) == 0:\n                 print(\"Warning: All clients scored 0.0 F1. Cannot rank. Using arbitrary order.\")\n                 ranked_client_ids = client_ids\n            else:\n                 sorted_clients_by_score = sorted(active_scores.items(), key=lambda kv: kv[1], reverse=True)\n                 ranked_client_ids = [cid for cid, _ in sorted_clients_by_score]\n            print(f\"New client training order: {ranked_client_ids}\")\n\n            print(\"\\nStep 2: Performing sequential local training (Conservative Jacobi-Seidel)...\")\n            current_iteration_local_weights = {}\n\n            f1_g, acc_g, _, _, _, _ = self._evaluate_model(self.reusable_model, W_global_prev, global_val_loader, \"Eval Global\")\n            score_global = f1_g\n            print(f\"Global Model Score (S_global = F1) for this round: {score_global:.4f}\")\n\n            for i, client_id in enumerate(tqdm(ranked_client_ids, desc=\"Sequential Client Training\", dynamic_ncols=True)):\n                weights_to_aggregate = []\n                coeffs_to_aggregate = []\n\n                # --- Conservative Jacobi-Seidel Logic ---\n                # 1. Add previous Global Model\n                weights_to_aggregate.append(W_global_prev)\n                coeffs_to_aggregate.append(score_global)\n\n                # 2. Add NEW models from peers trained THIS round (0 to i-1)\n                for j in range(i):\n                    peer_id = ranked_client_ids[j]\n                    if peer_id in current_iteration_local_weights:\n                        weights_to_aggregate.append(current_iteration_local_weights[peer_id])\n                        coeffs_to_aggregate.append(client_scores[peer_id]) \n\n                # 3. Add OLD models from self and peers NOT trained this round (i to N-1)\n                for j in range(i, len(ranked_client_ids)):\n                    peer_id = ranked_client_ids[j]\n                    if peer_id in historical_local_weights:\n                        weights_to_aggregate.append(historical_local_weights[peer_id])\n                        coeffs_to_aggregate.append(client_scores[peer_id]) # Use score from Step 1\n\n                # 4. Aggregate\n                w_initial = self._aggregate_weights(weights_to_aggregate, coeffs_to_aggregate)\n                if w_initial is None:\n                     print(f\"Warning: Aggregation failed for {client_id}. Using previous global model.\")\n                     w_initial = W_global_prev\n\n                # --- Train Local Model (MODIFIED: no current_round) ---\n                w_new_local = self._train_local_model(\n                    self.reusable_model,\n                    w_initial,\n                    client_dataloaders[client_id],\n                    client_id,\n                    epochs=self.LOCAL_EPOCHS\n                )\n                current_iteration_local_weights[client_id] = w_new_local\n\n                # --- Update Score Post-Training ---\n                if w_new_local is not None:\n                     f1, acc, _, _, _, _ = self._evaluate_model(self.reusable_model, w_new_local, global_val_loader, f\"Eval {client_id}\")\n                     score = f1\n                     client_scores[client_id] = score # Update score\n\n\n            print(\"\\nStep 3: Aggregating new local models to form the next global model...\")\n\n            final_local_weights_list = [current_iteration_local_weights.get(cid) for cid in client_ids]\n            aggregation_coeffs_list = [client_scores.get(cid, 0.0) for cid in client_ids]\n\n            W_global_new = self._aggregate_weights(final_local_weights_list, aggregation_coeffs_list)\n\n            if W_global_new is None:\n                print(\"\\nError: Final aggregation failed. Halting training.\")\n                break\n            else:\n                historical_local_weights = current_iteration_local_weights \n                W_global_prev = W_global_new \n\n                iter_f1, iter_acc, _, _, _, _ = self._evaluate_model(self.reusable_model, W_global_prev, global_val_loader)\n                print(f\"\\nEnd of Iteration {iteration+1}: Global Model F1={iter_f1:.4f}, Accuracy={iter_acc*100:.2f}%\")\n\n\n        # --- FINAL EVALUATION ---\n        if W_global_prev is None:\n             print(\"No final model available for evaluation.\")\n             return\n\n        print(f\"\\n{'='*25} FINAL RESULTS (Alpha = {self.ALPHA_VALUE}) {'='*25}\")\n\n        print(f\"\\n--- Evaluating Final Global Model ---\")\n        final_f1, final_acc, final_prc, final_rcl, y_true, y_pred = self._evaluate_model(\n            self.reusable_model, W_global_prev, self.final_test_loader, \"Final Global Test\"\n        )\n        print(f\"Overall Accuracy: {final_acc*100:.2f}%\")\n        print(f\"Overall F1-Score (Weighted): {final_f1:.4f}\")\n        print(f\"Overall Precision (Weighted): {final_prc:.4f}\")\n        print(f\"Overall Recall (Weighted): {final_rcl:.4f}\")\n\n        print(\"\\nClassification Report (Global Model):\")\n        print(classification_report(y_true, y_pred, target_names=self.class_names, digits=4, zero_division=0, labels=range(self.NUM_CLASSES)))\n\n        self._plot_confusion_matrix(y_true, y_pred, \"Confusion Matrix: Final Global Model - Test Set\")\n\n        print(f\"\\n--- Evaluating Final Local Models for each Client ---\")\n        for cid in client_ids: \n             final_local_weights = historical_local_weights.get(cid) \n             if final_local_weights is None:\n                   print(f\"\\n------------------ Client: {cid} (No final model available) ------------------\")\n                   continue\n\n             print(f\"\\n------------------ Client: {cid} ------------------\")\n             client_f1, client_acc, client_prc, client_rcl, y_true_client, y_pred_client = self._evaluate_model(\n                 self.reusable_model, final_local_weights, self.final_test_loader, f\"Final Test {cid}\"\n             )\n\n             print(f\"Overall Accuracy: {client_acc*100:.2f}%\")\n             print(f\"Overall F1-Score (Weighted): {client_f1:.4f}\")\n             print(f\"Overall Precision (Weighted): {client_prc:.4f}\")\n             print(f\"Overall Recall (Weighted): {client_rcl:.4f}\")\n\n             print(f\"\\nClassification Report ({cid}):\")\n             print(classification_report(y_true_client, y_pred_client, target_names=self.class_names, digits=4, zero_division=0, labels=range(self.NUM_CLASSES)))\n\n             self._plot_confusion_matrix(y_true_client, y_pred_client, f\"Confusion Matrix: Client {cid} - Test Set\")\n        print(f\"\\n{'='*75}\")\n\n# =============================================================================\n# MAIN EXECUTION BLOCK (Using torchvision CIFAR-10 for PSFL Comparison)\n# =============================================================================\nif __name__ == '__main__':\n    # --- Configuration ---\n    # IMG_SIZE = 32 (Native CIFAR-10 size, no resize needed for this CNN)\n    NUM_CLASSES = 10\n    CIFAR10_CLASS_NAMES = [\n        'airplane', 'automobile', 'bird', 'cat', 'deer',\n        'dog', 'frog', 'horse', 'ship', 'truck'\n    ]\n\n    # --- Transforms (Simpler, no resize, CIFAR-10 stats) ---\n    normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n\n    train_transform = transforms.Compose([\n        # NO transforms.Resize\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.ToTensor(),\n        normalize\n    ])\n\n    val_transform = transforms.Compose([\n        # NO transforms.Resize\n        transforms.ToTensor(),\n        normalize\n    ])\n\n    # --- Load Data using torchvision.datasets.CIFAR10 ---\n    print(\"Downloading/Loading CIFAR-10 train data using torchvision...\")\n    try:\n        full_train_dataset = torchvision.datasets.CIFAR10(\n            root='./data', train=True, download=True, transform=train_transform\n        )\n        print(f\"Full Train dataset size: {len(full_train_dataset)}\") # 50000\n\n        print(\"\\nDownloading/Loading CIFAR-10 test data using torchvision...\")\n        original_test_dataset = torchvision.datasets.CIFAR10(\n            root='./data', train=False, download=True, transform=val_transform\n        )\n        print(f\"Original Test dataset size: {len(original_test_dataset)}\") # 10000\n    except Exception as e:\n        print(f\"Error loading CIFAR-10: {e}\")\n        print(\"Please ensure internet is enabled if running in Kaggle/Colab.\")\n        exit() # Exit if dataset loading fails\n\n\n    # --- Create Train/Validation Split (80/20) ---\n    val_split_fraction = 0.2 # 10k validation\n    num_train_samples = len(full_train_dataset)\n    val_size = int(val_split_fraction * num_train_samples)\n    train_size = num_train_samples - val_size # 40k training\n    generator = torch.Generator().manual_seed(42)\n\n    print(f\"\\nSplitting train data into {train_size} train and {val_size} validation samples...\")\n    raw_train_dataset, raw_val_dataset = random_split(\n        full_train_dataset, [train_size, val_size], generator=generator\n    )\n\n    # --- Create Balanced Validation Set ---\n    print(\"\\n--- Creating Balanced Validation Dataset ---\")\n    val_dataset = create_median_balanced_subset(raw_val_dataset, NUM_CLASSES, random_seed=42)\n    print(\"-\" * 50)\n\n    # --- Federated Training Setup (Matching PSFL Paper) ---\n    base_config = {\n        \"NUM_GLOBAL_ITERATIONS\": 50, # PSFL paper used many rounds, start with 200\n        \"LOCAL_EPOCHS\": 5,          # Test E=5 (PSFL paper tested 1, 5, 10)\n        \"BATCH_SIZE\": 32,\n        \"LEARNING_RATE\": 0.01,      # LR from PSFL paper's grid search, tune this\n        \"RANDOM_SEED\": 42,\n        \"EPSILON\": 1e-9,\n        \"NUM_CLASSES\": NUM_CLASSES,\n        \"CLASS_NAMES\": CIFAR10_CLASS_NAMES\n    }\n\n    experiment_configs = [\n        # Run in a cross-silo setting (e.g., 20 clients, all participating)\n        {\"NUM_CLIENTS\": 20, \"ALPHA_VALUE\": 0.1},\n        #{\"NUM_CLIENTS\": 20, \"ALPHA_VALUE\": 0.5},\n    ]\n\n    for exp_conf in experiment_configs:\n        current_config = base_config.copy()\n        current_config.update(exp_conf)\n\n        print(f\"\\n\\n{'='*20} RUNNING LORENZO (Jacobi + Bootstrap + 5-Layer CNN) {'='*20}\")\n        print(f\"Config: {current_config}\")\n        \n        trainer = FederatedTrainer(\n            current_config,\n            raw_train_dataset,      # 40k split for client training\n            val_dataset,          # Balanced 10k for validation\n            original_test_dataset # Official 10k test set for final eval\n        )\n        trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}